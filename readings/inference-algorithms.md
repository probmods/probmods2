---
layout: exercise
title: "Algorithms for Inference - readings"
description: "MCMC, Gibbs, Metropolis Hastings, Particle Filters, Variational Bayes" 
---

## 1. Discussion of MCMC

@T.L.Griffiths:2008:dd194 - Sec. 5.0 Markov Chain Monte Carlo (pp. 31-34)

#### Reading questions:

a) Under what conditions is it *not* necessary to use an approximate sampling method to solve a Bayesian equation?

b) What are the major differences between Gibbs sampling and Metropolis-Hastings sampling? 

## 2. Particle filters

[Particle Filters Explained without Equations](https://www.youtube.com/watch?v=aUkBa1zMKv4)

#### Viewing questions:

a) As the number of particles increases, what happens to a particle filter's accuracy? What happens to its run-time? Would you want an infinite number of particles? Why or why not?

b) Describe a phenomenon that particle filters be particularly good for modeling. Why do you think a particle filter would be helpful?


## Extras
### Extra math
**Algorithms for Inference** For a somewhat longer, mathier disucssion of MCMC algorithms, see @andrieu2003introduction.

