---
layout: exercise
title: "Sequential decisions"
description: "Markov Decision Processes and Partially-Observable Markof Decision Processes" 
---

## Exercise 1

Consider our "line-world" example from the chapter:

'''js
var ___ = ' '; 
var D = { name: 'Donut' };

var grid = [
  ['___', '___', '___', '___',  D]
];

var mdp = makeGridWorldMDP({ grid, start: [0, 0] });

var transition = function(state, action) {
  return state + action;
};

var utility = function(state) {
  if (state === 4) {
    return 1;
  } else {
    return 0;
  }
};

var makeAgent = function() { 
  var act = function(state, timeLeft) {
    return Infer({ model() {
      var action = uniformDraw([-1, 0, 1]);
      var eu = expectedUtility(state, action, timeLeft);
      factor(100 * eu);
      return action;
    }});
  };

  var expectedUtility = function(state, action, timeLeft) {
    var u = utility(state, action);
    var newTimeLeft = timeLeft - 1;
    if (newTimeLeft === 0) {
      return u; 
    } else {
      return u + expectation(Infer({ model() {
        var nextState = transition(state, action); 
        var nextAction = sample(act(nextState, newTimeLeft));
        return expectedUtility(nextState, nextAction, newTimeLeft);
      }}));
    }
  };

  return { act };
}


var act = makeAgent().act;

var simulate = function(state, timeLeft){
  if (timeLeft === 0){
    return [];
  } else {
    var action = sample(act(state, timeLeft));
    var nextState = transition(state, action); 
    return [state].concat(simulate(nextState, timeLeft - 1))
  }
};

var startState = 0;
var totalTime = 5;
viz.gridworld(mdp.world, { trajectory : [mdp.startState] });
print("Agent's trajectory: " + simulate(startState, totalTime));
'''

### a) 
*Change the world such that it is a loop, i.e. moving right from state `4` moves to state `0`, and moving left from state `0` moves to state `4`. How does this change the agent's sequence of actions?*

Edit `transition()` to:

```js
var transition = function(state, action) {
  var nextstate = state + action
  return (nextstate < 0) ? 4 :
         (nextstate > 4) ? 0 :
         nextstate;
};
```

Agent now moves left to arrive at Donut shopt in a single move.

![](Figures/sequential-decisions-1.PNG)


### b) 
*Change the agent's action space such that the agent can also move two steps at a time. How does this change the agent's sequence of actions?*

Edit `act()` as follows:

```js
  var act = function(state, timeLeft) {
    return Infer({ model() {
      var action = uniformDraw([-2, -1, 0, 1, 2]);
      var eu = expectedUtility(state, action, timeLeft);
      factor(100 * eu);
      return action;
    }});
  };
```

Agent now only requires two moves to reach donut shop.

![](Figures/sequential-decisions-2.PNG)

### c) 
*Change the agent's utility function such that the agent moves as far as possible to the right, given its available total time.*

Edit `utility()` as follows:

```js
var utility = function(state) {
    return state;
};
```

Agent now moves right on every time step. This is easiest to see if we increase the total amount of time (e.g., `var totalTime = 7`):

![](Figures/sequential-decisions-3.PNG)

## Exercise 2

*Consider this "line-world" involving a cookie shop and a donut shop. Bob starts out in between the donut shop and the cookie shop. Assume you observe Bob go to the donut shop in 3 time steps. Edit the code above to write a model to *infer* Bob's utility function for cookies and donuts. Use any reasonable prior.* 

~~~~
// Anything that doesn't involve random choices can be put outside of the model

var ___ = ' ';
var D = { name: 'Donut' };
var C = { name: 'Cookie' };

  var grid = [
    [C, '___', '___', '___', '___', '___', D]
  ];

var mdp = makeGridWorldMDP({ grid, start: [3, 0] });

var transition = function(state, action) {
    return state + action;
  };

var model = function() {

  let utilities = [sample(Uniform({a: 0, b: 10})), sample(Uniform({a: 0, b: 10}))]
  var utility = function(state) {
    return (state == 0) ? utilities[0] :
           (state == 6) ? utilities[1] :
           0;
  };

  var makeAgent = function() {
    var act = function(state, timeLeft) {
      return Infer({ model() {
        var action = uniformDraw([-1, 0, 1]);
        var eu = expectedUtility(state, action, timeLeft);
        factor(100 * eu);
        return action;
      }});
    };

    var expectedUtility = function(state, action, timeLeft) {
      var u = utility(state, action);
      var newTimeLeft = timeLeft - 1;
      if (newTimeLeft === 0) {
        return u;
      } else {
        return u + expectation(Infer({ model() {
          var nextState = transition(state, action);
          var nextAction = sample(act(nextState, newTimeLeft));
          return expectedUtility(nextState, nextAction, newTimeLeft);
        }}));
      }
    };

    return { act };
  }

  var act = makeAgent().act;

  var simulate = function(state, timeLeft){
    if (timeLeft === 0){
      return [];
    } else {
      var action = sample(act(state, timeLeft));
      var nextState = transition(state, action);
      return [state].concat(simulate(nextState, timeLeft - 1))
    }
  };

  var startState = 3;
  var totalTime = 4;
  let path = simulate(startState, totalTime);
  condition(path[3] == 6);
  return {
    Cookie: utilities[0],
    Donut: utilities[1]
  }
 }

var post = Infer({method: 'MCMC', samples: 10000}, model)
viz(post);
~~~~

![](Figures/sequential-decisions-4.PNG)

Rejection sampling also works pretty well. This is with only 1,000 samples:

![](Figures/sequential-decisions-5.PNG)

Either way, we infer that the utility for Donut is likely to be at least slightly higher than that of Cookie.


## Exercise 3

*Use the codebox below to explore different levels of softmax noise. Find a setting of `utilityTable` and `alpha` such that the agent goes to West and East equally often and nearly always takes the most direct route to both East and West. Included below is code for simulating many trajectories and returning the trajectory length. You may find it helpful to extend this code to measure whether the route taken by the agent is direct or not.*

The following code is useful for iteratively adjusting the parameters until the desired result is found.

```js
///fold:
var makeHikeMDP = function(options) {
  var H = { name: 'Hill' };
  var W = { name: 'West' };
  var E = { name: 'East' };
  var ___ = ' ';
  var grid = [
    [___, ___, ___, ___, ___],
    [___, '#', ___, ___, ___],
    [___, '#',  W , '#',  E ],
    [___, ___, ___, ___, ___],
    [ H ,  H ,  H ,  H ,  H ]
  ];
  return makeGridWorldMDP(_.assign({ grid }, options));
};

var mdp = makeHikeMDP({
  start: [0, 1],
  totalTime: 13,
  transitionNoiseProbability: 0.1
});

var world = mdp.world;
var startState = mdp.startState;
var makeUtilityFunction = mdp.makeUtilityFunction;
viz.gridworld(world)
///


var utilityTable = {
  East: 10,
  West: 5.91,
  Hill: -10,
  timeCost: -1
}

var alpha = 5;  // <- SOFTMAX NOISE

// Create parameterized agent
var utility = makeUtilityFunction(utilityTable);
var agent = makeMDPAgent({ utility, alpha }, world);

var trajectories = Infer({model() {
    var trajectory = simulateMDP(startState, world, agent);
    var locs = map(function(v){return(v.loc)}, trajectory)
    return {locs}
  },
  method: 'forward',
  samples: 100000
});
viz.table(trajectories)
```

Note that the parameters given provide a nice result:




So we can definitely pick some values by trial and error. But that's boring. Let's infer it instead. The utility of West has to be less than the utility of East, or we'd never go to east. So let's fix the utility of East at 10 and find a value for West that is smaller. We'll also pick an alpha. Let's constrain it to between 0.1 and 6.0, just so we don't have too large of a space to search.

Now, we'll factor an equal number of times on having gone straight to West and having gone straight to East. 

```js
var makeHikeMDP = function(options) {
  var H = { name: 'Hill' };
  var W = { name: 'West' };
  var E = { name: 'East' };
  var ___ = ' ';
  var grid = [
    [___, ___, ___, ___, ___],
    [___, '#', ___, ___, ___],
    [___, '#',  W , '#',  E ],
    [___, ___, ___, ___, ___],
    [ H ,  H ,  H ,  H ,  H ]
  ];
  return makeGridWorldMDP(_.assign({ grid }, options));
};

var mdp = makeHikeMDP({
  start: [0, 1],
  totalTime: 13,
  transitionNoiseProbability: 0.1
});

var world = mdp.world;
var startState = mdp.startState;
var makeUtilityFunction = mdp.makeUtilityFunction;

viz.gridworld(world)
var vals = Infer({
  model() {
    var West = uniform({a: 1, b: 10})
    var utilityTable = {
      East: 10,
      West: West,
      Hill: -10,
      timeCost: -.1
    }

    // Create parameterized agent
    var utility = makeUtilityFunction(utilityTable);
    var alpha = uniform(0.1, 5);  // <- SOFTMAX NOISE
    var agent = makeMDPAgent({ utility, alpha }, world);
    repeat(10, function(){
      var trajectory = simulateMDP(startState, world, agent);
      var locs = map(function(v){return(v.loc)}, trajectory)
      factor(1*(locs == [[0,1],[1,1],[2,1],[2,2]]))
      var trajectory = simulateMDP(startState, world, agent);
      var locs = map(function(v){return(v.loc)}, trajectory)
      factor(1*(locs == [[0,1],[1,1],[2,1],[3,1],[4,1],[4,2]]))
       })
    return {West: West, alpha: alpha}
  },
  method: 'MCMC',
  samples: 5000
});
repeat(10,function(){print(sample(vals))})
```

![](Figures/sequential-decisions-6.PNG)

We can see that a value of West near 9.0 and alpha near 0.4 tends to work. Let's confirm this through forward simulation

```js
///fold:
var makeHikeMDP = function(options) {
  var H = { name: 'Hill' };
  var W = { name: 'West' };
  var E = { name: 'East' };
  var ___ = ' ';
  var grid = [
    [___, ___, ___, ___, ___],
    [___, '#', ___, ___, ___],
    [___, '#',  W , '#',  E ],
    [___, ___, ___, ___, ___],
    [ H ,  H ,  H ,  H ,  H ]
  ];
  return makeGridWorldMDP(_.assign({ grid }, options));
};

var mdp = makeHikeMDP({
  start: [0, 1],
  totalTime: 13,
  transitionNoiseProbability: 0.1
});

var world = mdp.world;
var startState = mdp.startState;
var makeUtilityFunction = mdp.makeUtilityFunction;
viz.gridworld(world)
///


var utilityTable = {
  East: 10,
  West: 3,
  Hill: -10,
  timeCost: -.1
}

var alpha = 0.4;  // <- SOFTMAX NOISE

// Create parameterized agent
var utility = makeUtilityFunction(utilityTable);
var agent = makeMDPAgent({ utility, alpha }, world);

var trajectories = Infer({model() {
    var trajectory = simulateMDP(startState, world, agent);
    var locs = map(function(v){return(v.loc)}, trajectory)
    return {locs}
  },
  method: 'forward',
  samples: 10000
});
viz.table(trajectories)
```